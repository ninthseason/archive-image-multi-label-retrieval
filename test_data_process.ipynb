{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-28T09:14:30.282571800Z",
     "start_time": "2023-08-28T09:14:30.263962900Z"
    }
   },
   "outputs": [],
   "source": [
    "# img_path = \"data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_path, input_transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.image_filenames = os.listdir(img_path)\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(osp.join(self.img_path, self.image_filenames[idx]))\n",
    "        if self.input_transform:\n",
    "            img = self.input_transform(img)\n",
    "        return img, self.image_filenames[idx]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T12:12:57.524416900Z",
     "start_time": "2023-08-28T12:12:55.076549600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "test_data_transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T12:31:38.810375800Z",
     "start_time": "2023-08-29T12:31:38.794249800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(\"data/test\", input_transform=test_data_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T12:13:09.579582300Z",
     "start_time": "2023-08-28T12:13:09.547052700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 2.0948,  2.0948,  2.1633,  ..., -2.0665, -2.0152, -2.0837],\n          [ 2.0605,  2.0777,  2.1462,  ..., -2.0837, -2.0323, -2.1008],\n          [ 1.9920,  2.0263,  2.1119,  ..., -2.1008, -2.0837, -2.1179],\n          ...,\n          [-2.1008, -2.0665, -2.0837,  ...,  1.8550,  1.8208,  1.9578],\n          [-2.1008, -2.0837, -2.1008,  ...,  1.8379,  1.8037,  1.8893],\n          [-2.1008, -2.0837, -2.1008,  ...,  1.8208,  1.8037,  1.8379]],\n \n         [[ 2.3235,  2.3060,  2.3585,  ..., -1.9832, -1.9307, -1.9657],\n          [ 2.2710,  2.2885,  2.3410,  ..., -2.0007, -1.9482, -1.9832],\n          [ 2.2010,  2.2360,  2.3235,  ..., -2.0182, -1.9832, -2.0182],\n          ...,\n          [-2.0007, -1.9832, -1.9832,  ...,  2.0609,  2.0259,  2.1485],\n          [-2.0007, -2.0007, -2.0007,  ...,  2.0259,  2.0084,  2.0784],\n          [-2.0007, -2.0007, -2.0182,  ...,  2.0084,  1.9909,  2.0259]],\n \n         [[ 2.3786,  2.4134,  2.5703,  ..., -1.7696, -1.7347, -1.7870],\n          [ 2.3786,  2.4134,  2.5354,  ..., -1.7870, -1.7522, -1.7870],\n          [ 2.3611,  2.3960,  2.4831,  ..., -1.8044, -1.7870, -1.8044],\n          ...,\n          [-1.8044, -1.7696, -1.6999,  ...,  2.2914,  2.2391,  2.4308],\n          [-1.8044, -1.7696, -1.7173,  ...,  2.2566,  2.2217,  2.3611],\n          [-1.8044, -1.7696, -1.7347,  ...,  2.2391,  2.2217,  2.3088]]]),\n '00052fbb-bc66-4981-a5b8-657cece65474.jpg')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T12:13:10.902409800Z",
     "start_time": "2023-08-28T12:13:10.836187Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def parser_args(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Query2Label MSCOCO Training')\n",
    "    parser.add_argument('--dataname', help='dataname', default='my', choices=['coco14', 'my'])\n",
    "    parser.add_argument('--dataset_dir', help='dir of dataset', default='/comp_robot/liushilong/data/COCO14/')\n",
    "    parser.add_argument('--img_size', default=448, type=int,\n",
    "                        help='size of input images')\n",
    "\n",
    "    parser.add_argument('--output', metavar='DIR',\n",
    "                        help='path to output folder')\n",
    "    parser.add_argument('--num_class', default=4375, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                        help='use pre-trained model. default is False. ')\n",
    "    parser.add_argument('--optim', default='AdamW', type=str, choices=['AdamW', 'Adam_twd'],\n",
    "                        help='which optim to use')\n",
    "\n",
    "    # loss\n",
    "    parser.add_argument('--eps', default=1e-5, type=float,\n",
    "                        help='eps for focal loss (default: 1e-5)')\n",
    "    parser.add_argument('--dtgfl', action='store_true', default=False,\n",
    "                        help='disable_torch_grad_focal_loss in asl')\n",
    "    parser.add_argument('--gamma_pos', default=0, type=float,\n",
    "                        metavar='gamma_pos', help='gamma pos for simplified asl loss')\n",
    "    parser.add_argument('--gamma_neg', default=2, type=float,\n",
    "                        metavar='gamma_neg', help='gamma neg for simplified asl loss')\n",
    "    parser.add_argument('--loss_dev', default=-1, type=float,\n",
    "                        help='scale factor for loss')\n",
    "    parser.add_argument('--loss_clip', default=0.0, type=float,\n",
    "                        help='scale factor for clip')\n",
    "\n",
    "    parser.add_argument('-j', '--workers', default=0, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 32)')\n",
    "    parser.add_argument('--epochs', default=80, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "\n",
    "    parser.add_argument('--val_interval', default=1, type=int, metavar='N',\n",
    "                        help='interval of validation')\n",
    "\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N',\n",
    "                        help='mini-batch size (default: 256), this is the total '\n",
    "                             'batch size of all GPUs')\n",
    "\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                        metavar='LR', help='initial learning rate', dest='lr')\n",
    "    parser.add_argument('--wd', '--weight-decay', default=1e-2, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-2)',\n",
    "                        dest='weight_decay')\n",
    "\n",
    "    parser.add_argument('-p', '--print-freq', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--resume_omit', default=[], type=str, nargs='*')\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                        help='evaluate model on validation set')\n",
    "\n",
    "    parser.add_argument('--ema-decay', default=0.9997, type=float, metavar='M',\n",
    "                        help='decay of model ema')\n",
    "    parser.add_argument('--ema-epoch', default=0, type=int, metavar='M',\n",
    "                        help='start ema epoch')\n",
    "\n",
    "    # distribution training\n",
    "    parser.add_argument('--world-size', default=-1, type=int,\n",
    "                        help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=-1, type=int,\n",
    "                        help='node rank for distributed training')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--seed', default=None, type=int,\n",
    "                        help='seed for initializing training. ')\n",
    "    parser.add_argument(\"--local_rank\", type=int, help='local rank for DistributedDataParallel')\n",
    "\n",
    "    # data aug\n",
    "    parser.add_argument('--cutout', action='store_true', default=False,\n",
    "                        help='apply cutout')\n",
    "    parser.add_argument('--n_holes', type=int, default=1,\n",
    "                        help='number of holes to cut out from image')\n",
    "    parser.add_argument('--length', type=int, default=-1,\n",
    "                        help='length of the holes. suggest to use default setting -1.')\n",
    "    parser.add_argument('--cut_fact', type=float, default=0.5,\n",
    "                        help='mutual exclusion with length. ')\n",
    "\n",
    "    parser.add_argument('--orid_norm', action='store_true', default=False,\n",
    "                        help='using mean [0,0,0] and std [1,1,1] to normalize input images')\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=1, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=2, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=8192, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=2048, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=4, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--pre_norm', action='store_true')\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "    parser.add_argument('--backbone', default='resnet101', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--keep_other_self_attn_dec', action='store_true',\n",
    "                        help='keep the other self attention modules in transformer decoders, which will be removed default.')\n",
    "    parser.add_argument('--keep_first_self_attn_dec', action='store_true',\n",
    "                        help='keep the first self attention module in transformer decoders, which will be removed default.')\n",
    "    parser.add_argument('--keep_input_proj', action='store_true',\n",
    "                        help=\"keep the input projection layer. Needed when the channel of image features is different from hidden_dim of Transformer layers.\")\n",
    "\n",
    "    # * raining\n",
    "    parser.add_argument('--amp', action='store_true', default=False,\n",
    "                        help='apply amp')\n",
    "    parser.add_argument('--early-stop', action='store_true', default=False,\n",
    "                        help='apply early stop')\n",
    "    parser.add_argument('--kill-stop', action='store_true', default=False,\n",
    "                        help='apply early stop')\n",
    "    args = parser.parse_args(args)\n",
    "    return args\n",
    "\n",
    "def get_args(args=None):\n",
    "    args = parser_args(args)\n",
    "    return args"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T12:26:58.847812700Z",
     "start_time": "2023-08-29T12:26:58.834780700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Namespace(dataname='my', dataset_dir='data', img_size=448, output='\"result\"', num_class=4375, pretrained=True, optim='AdamW', eps=1e-05, dtgfl=True, gamma_pos=0.0, gamma_neg=2.0, loss_dev=-1, loss_clip=0.0, workers=0, epochs=80, val_interval=1, start_epoch=0, batch_size=2, lr=0.0001, weight_decay=0.01, print_freq=100, resume='', resume_omit=[], evaluate=False, ema_decay=0.9997, ema_epoch=0, world_size=1, rank=0, dist_url='tcp://127.0.0.1:3717', seed=None, local_rank=None, cutout=True, n_holes=1, length=-1, cut_fact=0.5, orid_norm=False, enc_layers=1, dec_layers=2, dim_feedforward=8192, hidden_dim=2048, dropout=0.1, nheads=4, pre_norm=False, position_embedding='sine', backbone='resnet101', keep_other_self_attn_dec=False, keep_first_self_attn_dec=False, keep_input_proj=False, amp=True, early_stop=True, kill_stop=False)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = get_args('--dataset_dir data --backbone resnet101 --dataname my --batch-size 2 --print-freq 100 --output \"result\" --world-size 1 --rank 0 --dist-url tcp://127.0.0.1:3717 --gamma_pos 0 --gamma_neg 2 --dtgfl --epochs 80 --lr 1e-4 --optim AdamW --pretrained --num_class 4375 --img_size 448 --weight-decay 1e-2 --cutout --n_holes 1 --cut_fact 0.5 --hidden_dim 2048 --dim_feedforward 8192 --enc_layers 1 --dec_layers 2 --nheads 4 --early-stop --amp'.split())\n",
    "args"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T12:27:03.961571900Z",
     "start_time": "2023-08-29T12:27:03.945249700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set model.input_proj to Indentify!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from models.query2label import build_q2l\n",
    "import torch\n",
    "model = build_q2l(args)\n",
    "checkpoint = torch.load(\"query2labels/result/model_best.pth.tar\")\n",
    "\n",
    "def clean_state_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k[:7] == 'module.':\n",
    "            k = k[7:]  # remove `module.`\n",
    "        new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "state_dict = clean_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T12:29:12.883301900Z",
     "start_time": "2023-08-29T12:29:07.931599300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T12:29:19.681043100Z",
     "start_time": "2023-08-29T12:29:19.117777100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from dataset.cocodataset import CoCoDataset\n",
    "\n",
    "train_ds = CoCoDataset(\"\", \"\", input_transform=test_data_transform, labels_path=\"query2labels/data/coco/train_label_vectors.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T13:06:28.444015500Z",
     "start_time": "2023-08-29T13:06:27.835861500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"category_map.json\", \"r\") as f:\n",
    "    cate = json.load(f)\n",
    "\n",
    "r_cate = {cate[i]: i for i in cate}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T13:06:33.873313Z",
     "start_time": "2023-08-29T13:06:33.858658400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.where(train_ds[0][1] == 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T13:06:39.198949500Z",
     "start_time": "2023-08-29T13:06:39.185315800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "舞台\n",
      "地站\n",
      "话筒\n",
      "男人\n"
     ]
    }
   ],
   "source": [
    "for i in a[0]:\n",
    "    print(r_cate[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-29T13:06:44.395401900Z",
     "start_time": "2023-08-29T13:06:44.370151400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dl = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# for img, name in test_dl:\n",
    "#     print(img)\n",
    "#     print(name)\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T12:13:25.952735800Z",
     "start_time": "2023-08-28T12:13:25.914709900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "test_label = pd.DataFrame(columns=['filename', 'vector'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, name in tqdm(test_dl):\n",
    "        img = img.to(\"cuda\")\n",
    "        result = model(img).cpu().numpy()\n",
    "        for idx, filename in enumerate(name):\n",
    "            test_label = test_label.append({'filename': filename, 'vector': list(result[idx])}, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#\n",
    "# test_label = pd.DataFrame(columns=['filename', 'vector'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T09:43:17.484737700Z",
     "start_time": "2023-08-28T09:43:17.366000100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# test_label = test_label.append({'filename': \"a.jpg\"}, ignore_index=True)\n",
    "test_label.to_csv(\"test_label.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
